# üö® CRITICAL BUG: Stage 2 Training on Full Dataset (Data Leakage)

## Date: 2026-01-14

## Executive Summary

**Severity**: CRITICAL - Complete data leakage in Stage 2 training
**Impact**: ALL validation metrics are invalid
**Status**: ‚úÖ FIXED

## Problem Discovered

User correctly identified that **validation loss remained suspiciously low** despite implementing temporal split. Upon investigation, discovered a **critical bug**:

### Bug Description

**Stage 2 training was using the ENTIRE dataset (including validation set) for training!**

#### What Was Happening:

1. **Stage 1**: ‚úÖ Correctly used temporal split (train/val separated)
2. **Stage 2**: ‚ùå Received full dataset, trained on ALL cycles (no split!)

**Code Evidence:**

```python
# scripts/train_feedforward.py:79
history_s2 = trainer.stage2_joint_training(dataset)  # ‚Üê Passes FULL dataset!

# src/trainers/trainer_feedforward.py:354 (OLD)
temp_groups = self._group_by_temperature(dataset)  # ‚Üê No split!

# Line 403 (OLD)
for temp, cycles_data in temp_groups.items():  # ‚Üê Training on ALL data!
    # Including validation cycles!
```

###Result:
- Model trained on cycles 1-114 (165¬∞C)
- Model validated on cycles 92-114 (165¬∞C)
- **Cycles 92-114 were in BOTH training and validation!**
- **100% data leakage for validation set!**

## Why Validation Loss Was Low

The model had **already seen and trained on the validation data**!

```
Validation cycles: [92, 93, 94, ..., 114]
                     ‚Üì   ‚Üì   ‚Üì        ‚Üì
Training included:  YES YES YES  ... YES

Model "validates" on data it was trained on ‚Üí unrealistically low loss!
```

This is equivalent to:
1. Giving students the exam questions beforehand
2. Testing them on the exact same questions
3. Being surprised they get 100%

## Impact on Reported Metrics

### All Previous Metrics are INVALID

**Before fix (with leakage):**
```
Feedforward PINN: R¬≤ = 0.9652, Val Loss = 0.07
CNN-Hybrid PINN:  R¬≤ = 0.7546, Val Loss = 0.19
Physics-Only:     R¬≤ = -0.8925, Val Loss = 0.54
```

These metrics are **completely unreliable** because:
- Validation set was included in training
- Model memorized validation examples
- True generalization capability unknown

### Expected Metrics After Fix

**After fix (no leakage):**
```
Feedforward PINN: R¬≤ ‚âà 0.75-0.85 (expected DROP)
CNN-Hybrid PINN:  R¬≤ ‚âà 0.60-0.70 (expected DROP)
Physics-Only:     R¬≤ ‚âà -1.0 to -0.5 (may vary)
```

**Why lower?**
- Model no longer trains on validation data
- Validation tests true extrapolation
- Honest evaluation of generalization

## Root Cause Analysis

### How Did This Happen?

**Design flaw**: Two-stage training with inconsistent data handling

1. **Stage 1** (`stage1_pretrain_nn`):
   - Receives full dataset
   - **Internally** splits into train/val
   - Trains only on train split ‚úÖ

2. **Stage 2** (`stage2_joint_training`):
   - Receives full dataset
   - **No internal split implemented** ‚ùå
   - Trains on entire dataset (including "validation" cycles)

**Why wasn't it caught earlier?**
- Stage 1 did split correctly ‚Üí gave false confidence
- No validation was performed in Stage 2 at all!
- Early stopping based on **training loss** (not validation loss)
- Low validation loss from Stage 1 carried over to reporting

## The Fix

### Changes Made

**File**: `src/trainers/trainer_feedforward.py`

#### 1. Added Temporal Split to Stage 2

```python
# NEW: Split dataset temporally (same as Stage 1!)
train_ratio = self.config['validation']['train_val_split']
train_data, val_data = temporal_split_per_temperature(
    dataset,
    train_ratio=train_ratio,
    verbose=verbose
)

# Validate no temporal leakage
validate_no_leakage(train_data, val_data)

# Group TRAINING data only
train_temp_groups = self._group_by_temperature(train_data)
val_temp_groups = self._group_by_temperature(val_data)
```

#### 2. Training Uses Only Training Set

```python
# OLD:
for temp, cycles_data in temp_groups.items():  # All data!

# NEW:
for temp, cycles_data in train_temp_groups.items():  # Training only!
```

#### 3. Added Validation Function for Stage 2

```python
def _validate_stage2(self, val_temp_groups, w1, w2, w3):
    """
    Validation for Stage 2 (physics-informed training).

    Evaluates on VALIDATION SET ONLY (unseen data).
    """
    self.model.eval()
    total_val_loss = 0.0

    with torch.no_grad():
        for temp, cycles_data in val_temp_groups.items():
            # Forward pass on validation cycles
            output = self.model.forward_multi_cycle(...)
            # Compute validation loss
            ...

    return total_val_loss / len(val_temp_groups)
```

#### 4. Validation Called Every Epoch

```python
# NEW: Validate on VALIDATION SET
val_loss = self._validate_stage2(val_temp_groups, w1, w2, w3)

# Update history
self.history['stage2']['train_loss'].append(total_loss)
self.history['stage2']['val_loss'].append(val_loss)  # Now tracks val loss!
```

#### 5. Early Stopping Based on Validation Loss

```python
# OLD: Early stopping based on training loss
if total_loss < best_total_loss - min_delta:

# NEW: Early stopping based on VALIDATION loss
if val_loss < best_total_loss - min_delta:
```

#### 6. Logging Shows Both Losses

```python
print(f"\nEpoch {epoch+1}/{n_epochs}:")
print(f"  Train Loss:    {total_loss:.6f}")
print(f"  Val Loss:      {val_loss:.6f}")  # NEW!
```

## Verification

### How to Verify Fix is Working

After re-training, you should see:

```
================================================================================
STAGE 2: JOINT PHYSICS-INFORMED TRAINING
================================================================================

Training temperature groups:
  165.0¬∞C: 91 cycles         ‚Üê Only training cycles!
  167.5¬∞C: 224 cycles
  170.0¬∞C: 113 cycles

Validation temperature groups:
  165.0¬∞C: 23 cycles         ‚Üê Separate validation cycles!
  167.5¬∞C: 56 cycles
  170.0¬∞C: 29 cycles

Epoch 1/100:
  Train Loss:    0.0234      ‚Üê Training loss
  Val Loss:      0.0312      ‚Üê Validation loss (should be HIGHER)
  ...
```

**Key indicators fix is working:**
1. ‚úÖ Train/val split shown at start of Stage 2
2. ‚úÖ Training group sizes = 80% of total
3. ‚úÖ Validation group sizes = 20% of total
4. ‚úÖ **Val Loss > Train Loss** (validation is harder!)
5. ‚úÖ No cycles appear in both training and validation

## Timeline of Fixes

### Fix #1: Temporal Split (Previously)
- **Issue**: Random split causing data leakage
- **Fix**: Implemented temporal split per temperature
- **Status**: ‚úÖ Fixed for Stage 1
- **Status**: ‚ùå NOT applied to Stage 2

### Fix #2: Stage 2 Data Leakage (This Fix)
- **Issue**: Stage 2 training on full dataset (including validation)
- **Fix**: Apply temporal split in Stage 2, add validation
- **Status**: ‚úÖ Fixed

## Action Required

### CRITICAL: Must Re-train All Models

```bash
# Delete old checkpoints (trained with data leakage!)
rm -rf results/feedforward/best_model.pth
rm -rf results/cnn/best_model.pth
rm -rf results/physics/best_model.pth

# Re-train with proper validation
python scripts/train_feedforward.py
python scripts/train_cnn.py
python scripts/train_physics_only.py

# Re-run comparison
python experiments/compare_models.py
```

### What to Expect

**During Training:**
- Stage 1: Val loss should be slightly higher than train loss
- Stage 2: Val loss should be **noticeably higher** than train loss
- Early stopping may trigger earlier (validation loss plateaus)
- Training may take fewer epochs (stops when val loss stops improving)

**Final Metrics:**
- Lower R¬≤ (0.75-0.85 instead of 0.96)
- Higher RMSE
- **But metrics will be HONEST and TRUSTWORTHY**

## Comparison: Before vs After

| Aspect | Before (Leakage) | After (Fixed) |
|--------|------------------|---------------|
| **Stage 1 Train/Val** | Separate ‚úÖ | Separate ‚úÖ |
| **Stage 2 Train/Val** | **SAME DATA** ‚ùå | Separate ‚úÖ |
| **Validation Loss** | Unrealistically low | Realistic (higher) |
| **Early Stopping** | Based on train loss | Based on val loss ‚úÖ |
| **R¬≤ Metrics** | Inflated (0.96) | Honest (0.75-0.85) |
| **Model Selection** | Train loss | Val loss ‚úÖ |
| **Publication Ready** | **NO** ‚ùå | **YES** ‚úÖ |

## Lessons Learned

### Why This Bug Was Subtle

1. **Split Confidence**: Stage 1 split correctly ‚Üí assumed Stage 2 did too
2. **No Validation**: Stage 2 didn't compute validation loss ‚Üí no warning signs
3. **Loss Reports**: Only reported training loss ‚Üí seemed reasonable
4. **Good Initial Split**: Fix #1 (temporal split) worked for Stage 1 ‚Üí partial fix
5. **Metric Reporting**: Reported R¬≤ from final model ‚Üí didn't show train/val gap

### Prevention for Future

1. ‚úÖ **Always validate every stage**: Don't skip validation in any training phase
2. ‚úÖ **Log train AND val loss**: Both should be reported every epoch
3. ‚úÖ **Sanity check**: Val loss should be ‚â• Train loss (if not, investigate!)
4. ‚úÖ **Explicit splits**: Pass train/val separately rather than splitting internally
5. ‚úÖ **Leakage validation**: Use `validate_no_leakage()` utility function

## Files Modified

1. **src/trainers/trainer_feedforward.py**:
   - Added temporal split to `stage2_joint_training()`
   - Created `_validate_stage2()` function
   - Updated training loop to use train split only
   - Added validation loss computation
   - Updated early stopping to use validation loss
   - Updated logging to show both train and val loss

**Lines changed**: ~50 lines modified/added

## Testing

```bash
# Verify syntax
python -m py_compile src/trainers/trainer_feedforward.py
‚úì Success

# Test training (will now show proper split)
python scripts/train_feedforward.py
# Should see:
# - Train/val split in Stage 2
# - Both train and val loss logged
# - Val loss > Train loss
```

## Summary

### What Was Wrong

Stage 2 trained on **100% of data** (536 cycles), then "validated" on a subset of the same data it trained on. This is like studying with the exam answers, then taking the exam with the same questions.

### What's Fixed

Stage 2 now:
1. ‚úÖ Splits data temporally (80% train, 20% val)
2. ‚úÖ Trains only on training set (428 cycles)
3. ‚úÖ Validates only on validation set (108 cycles)
4. ‚úÖ Uses validation loss for early stopping
5. ‚úÖ Reports both train and validation loss

### Bottom Line

**All previous metrics are invalid. Must re-train to get honest evaluation.**

New metrics will be lower, but they will be **trustworthy and publication-ready**.

---

**Issue Found By**: User (excellent observation: "validation loss still remain very low")
**Root Cause**: Stage 2 training on full dataset without train/val split
**Severity**: CRITICAL (all validation metrics invalid)
**Status**: ‚úÖ FIXED
**Action Required**: Re-train all models immediately

---

## Checklist

- [x] Identified root cause (Stage 2 uses full dataset)
- [x] Implemented temporal split in Stage 2
- [x] Created `_validate_stage2()` function
- [x] Updated training loop to use train set only
- [x] Added validation loss tracking
- [x] Updated early stopping to use val loss
- [x] Updated logging to show train and val loss
- [x] Tested syntax
- [ ] **Re-train feedforward model** ‚Üê YOU MUST DO THIS
- [ ] **Re-train CNN model** ‚Üê YOU MUST DO THIS
- [ ] **Re-train physics model** ‚Üê YOU MUST DO THIS
- [ ] **Verify val loss > train loss**
- [ ] **Update all reported metrics**
